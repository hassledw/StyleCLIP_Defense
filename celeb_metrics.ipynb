{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization and Metrics on the CelebA_HQ Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate Dataframes\n",
    "\n",
    "Each dataframe consists of 5 columns: `image`, `image_path`, `actual_label`, `predicted_label`, `confidence`\n",
    "* `image` is the image name, for example 10.jpg\n",
    "* `image_path` is the path to the image\n",
    "* `actual_label` is the actual label for that person (the original image)\n",
    "* `predicted_label` is the result of running the original, attacked, or defended image into the classifier. This is the label used to determine if the attack or defense was effective.\n",
    "* `confidence` is the confidence in which we think that the image belongs to that class. This is the max value of the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgsm05_attack_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/FGSM05.csv\")\n",
    "fgsm05_defend_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-FGSM05.csv\")\n",
    "fgsm10_attack_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/FGSM10.csv\")\n",
    "fgsm10_defend_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-FGSM10.csv\")\n",
    "pgd1010_attack_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/PGD1010.csv\")\n",
    "pgd1010_defend_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-PGD1010.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defense Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_defense_metrics(attackname, attack_df, defense_df, total_images=199):\n",
    "   # Successful attacks\n",
    "   attack_result = attack_df[attack_df[\"actual_label\"] != attack_df[\"predicted_label\"]]\n",
    "   # Successful defends inclusive of unsuccessful misclassfication in the attacks.\n",
    "   defense_result = defense_df[defense_df[\"actual_label\"] == defense_df[\"predicted_label\"]]\n",
    "   # Attacked image detection with StyleCLIP.\n",
    "   attack_detection = total_images - len(defense_df)\n",
    "   # Successful attacks and defenses joined. _x data is the attack, _y is the defense.\n",
    "   attack_defense_join = pd.merge(attack_result, defense_df, on=['image'], how='inner').drop(columns=['Unnamed: 0_x', 'image_path_x', 'confidence_x', 'Unnamed: 0_y', 'image_path_y',\n",
    "      'actual_label_y'])\n",
    "   # Successfully reclassified images after successful attack.\n",
    "   attack_defense_success = attack_defense_join[attack_defense_join[\"actual_label_x\"] == attack_defense_join[\"predicted_label_y\"]]\n",
    "   # Instances when the defense was too weak. When a successful attack happened, the defense generated an image with the same label as the attack.\n",
    "   attack_defense_weak = attack_defense_join[attack_defense_join[\"predicted_label_x\"] == attack_defense_join[\"predicted_label_y\"]]\n",
    "\n",
    "   print(\"-\" * 50)\n",
    "   print(f\"RUNNING {attackname} DEFENSE METRICS...\\n\")\n",
    "   print(f\"Attack Effectiveness: {len(attack_result) / len(attack_df) * 100:.2f}%\\n\")\n",
    "\n",
    "   print(f\"StyleCLIP Sanitization Effectiveness (on ALL defendable images): {len(defense_result) / len(defense_df) * 100:.2f}%\")\n",
    "   print(f\"StyleCLIP Sanitization Effectiveness (on successful attacks): {len(attack_defense_success) / len(attack_defense_join) * 100:.2f}%\")\n",
    "   print(f\"Attacked Image Detection w/ StyleCLIP: {attack_detection / total_images * 100:.2f}%\")\n",
    "   print(f\"Misclassification Caused by StyleCLIP: {100 - (len(defense_result) / len(defense_df) * 100):.2f}%\")\n",
    "   print(f\"Successful Attacks Where Defense TOO Weak: {len(attack_defense_weak) / len(attack_defense_join) * 100:.2f}%\")\n",
    "   print(f\"Total StyleCLIP Dataset Retention: {(attack_detection + len(defense_result)) / total_images * 100:.2f}%\")\n",
    "   print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "RUNNING FGSM05 DEFENSE METRICS...\n",
      "\n",
      "Attack Effectiveness: 24.12%\n",
      "\n",
      "StyleCLIP Sanitization Effectiveness (on ALL defendable images): 50.00%\n",
      "StyleCLIP Sanitization Effectiveness (on successful attacks): 11.11%\n",
      "Attacked Image Detection w/ StyleCLIP: 1.51%\n",
      "Misclassification Caused by StyleCLIP: 50.00%\n",
      "Successful Attacks Where Defense TOO Weak: 44.44%\n",
      "Total StyleCLIP Dataset Retention: 50.75%\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "RUNNING FGSM10 DEFENSE METRICS...\n",
      "\n",
      "Attack Effectiveness: 50.25%\n",
      "\n",
      "StyleCLIP Sanitization Effectiveness (on ALL defendable images): 27.04%\n",
      "StyleCLIP Sanitization Effectiveness (on successful attacks): 5.15%\n",
      "Attacked Image Detection w/ StyleCLIP: 1.51%\n",
      "Misclassification Caused by StyleCLIP: 72.96%\n",
      "Successful Attacks Where Defense TOO Weak: 52.58%\n",
      "Total StyleCLIP Dataset Retention: 28.14%\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "RUNNING PGD1010 DEFENSE METRICS...\n",
      "\n",
      "Attack Effectiveness: 44.22%\n",
      "\n",
      "StyleCLIP Sanitization Effectiveness (on ALL defendable images): 34.01%\n",
      "StyleCLIP Sanitization Effectiveness (on successful attacks): 6.98%\n",
      "Attacked Image Detection w/ StyleCLIP: 1.01%\n",
      "Misclassification Caused by StyleCLIP: 65.99%\n",
      "Successful Attacks Where Defense TOO Weak: 45.35%\n",
      "Total StyleCLIP Dataset Retention: 34.67%\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_defense_metrics(\"FGSM05\", fgsm05_attack_df, fgsm05_defend_df)\n",
    "print_defense_metrics(\"FGSM10\", fgsm10_attack_df, fgsm10_defend_df)\n",
    "print_defense_metrics(\"PGD1010\", pgd1010_attack_df, pgd1010_defend_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
