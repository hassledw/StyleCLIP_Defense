{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization and Metrics on the CelebA_HQ Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate Dataframes\n",
    "\n",
    "Each dataframe consists of 5 columns: `image`, `image_path`, `actual_label`, `predicted_label`, `confidence`\n",
    "* `image` is the image name, for example 10.jpg\n",
    "* `image_path` is the path to the image\n",
    "* `actual_label` is the actual label for that person (the original image)\n",
    "* `predicted_label` is the result of running the original, attacked, or defended image into the classifier. This is the label used to determine if the attack or defense was effective.\n",
    "* `confidence` is the confidence in which we think that the image belongs to that class. This is the max value of the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgsm05_attack_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/FGSM05.csv\")\n",
    "fgsm05_defend_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-FGSM05.csv\")\n",
    "fgsm05_detected_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-FGSM05-detected.csv\")\n",
    "\n",
    "fgsm10_attack_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/FGSM10.csv\")\n",
    "fgsm10_defend_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-FGSM10.csv\")\n",
    "fgsm10_detected_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-FGSM10-detected.csv\")\n",
    "\n",
    "pgd1010_attack_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/PGD1010.csv\")\n",
    "pgd1010_defend_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-PGD1010.csv\")\n",
    "pgd1010_detected_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-PGD1010-detected.csv\")\n",
    "\n",
    "pgd2010_attack_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/PGD2010.csv\")\n",
    "pgd2010_defend_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-PGD2010.csv\")\n",
    "pgd2010_detected_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-PGD2010-detected.csv\")\n",
    "\n",
    "fgsm50_attack_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/FGSM50.csv\")\n",
    "fgsm50_defend_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-FGSM50.csv\")\n",
    "fgsm50_detected_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-FGSM50-detected.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defense Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_defense_metrics(attackname, attack_df, defense_df, detected_df):\n",
    "   total_images = len(attack_df)\n",
    "   \n",
    "   # Attacked image detection with StyleCLIP.\n",
    "   attack_detection = pd.merge(detected_df, defense_df, on=['image'], how='inner').drop(columns=['Unnamed: 0_x', 'image_path_x', 'Unnamed: 0_y', 'image_path_y',\n",
    "       'actual_label', 'predicted_label', 'confidence'])\n",
    "   \n",
    "   # Filter out detected images in defended_df dataset.\n",
    "   defense_df = pd.merge(detected_df, defense_df, on=['image'], how='outer')\n",
    "   defense_df = defense_df[defense_df.isna().any(axis=1)].drop(columns=['Unnamed: 0_x', 'image_path_x', 'Unnamed: 0_y'])\n",
    "   defense_df = defense_df.rename(columns={'image_path_y': 'image_path'})\n",
    "   # Successful attacks\n",
    "   attack_result = attack_df[attack_df[\"actual_label\"] != attack_df[\"predicted_label\"]]\n",
    "\n",
    "   # Successful defends inclusive of unsuccessful misclassfication in the attacks.\n",
    "   defense_result = defense_df[defense_df[\"actual_label\"] == defense_df[\"predicted_label\"]]\n",
    "\n",
    "   # Successful attacks and defenses joined. _x data is the attack, _y is the defense.\n",
    "   attack_defense_join = pd.merge(attack_result, defense_df, on=['image'], how='inner')\n",
    "\n",
    "   # Successfully reclassified images after successful attack.\n",
    "   attack_defense_success = attack_defense_join[attack_defense_join[\"actual_label_x\"] == attack_defense_join[\"predicted_label_y\"]]\n",
    "   \n",
    "   # Instances when the defense was too weak. When a successful attack happened, the defense generated an image with the same label as the attack.\n",
    "   attack_defense_weak = attack_defense_join[attack_defense_join[\"predicted_label_x\"] == attack_defense_join[\"predicted_label_y\"]]\n",
    "\n",
    "   print(\"-\" * 50)\n",
    "   print(f\"RUNNING {attackname} DEFENSE METRICS...\\n\")\n",
    "   print(f\"Attack Effectiveness: {len(attack_result) / len(attack_df) * 100:.2f}%\\n\")\n",
    "\n",
    "   print(f\"StyleCLIP Sanitization Effectiveness (on ALL defendable images): {len(defense_result) / len(defense_df) * 100:.2f}%\")\n",
    "   print(f\"StyleCLIP Sanitization Effectiveness (on successful attacks): {len(attack_defense_success) / len(attack_defense_join) * 100:.2f}%\")\n",
    "   print(f\"Attacked Image Detection w/ StyleCLIP: {len(attack_detection) / total_images * 100:.2f}%\")\n",
    "   print(f\"Misclassification on StyleCLIP Images: {100 - (len(defense_result) / len(defense_df) * 100):.2f}%\")\n",
    "   print(f\"Successful Attacks Where Defense TOO Weak: {len(attack_defense_weak) / len(attack_defense_join) * 100:.2f}%\")\n",
    "   print(f\"Total StyleCLIP Dataset Retention: {(len(attack_detection) + len(defense_result)) / total_images * 100:.2f}%\")\n",
    "   print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "RUNNING FGSM50 DEFENSE METRICS...\n",
      "\n",
      "Attack Effectiveness: 96.48%\n",
      "\n",
      "StyleCLIP Sanitization Effectiveness (on ALL defendable images): 0.00%\n",
      "StyleCLIP Sanitization Effectiveness (on successful attacks): 0.00%\n",
      "Attacked Image Detection w/ StyleCLIP: 54.27%\n",
      "Misclassification on StyleCLIP Images: 100.00%\n",
      "Successful Attacks Where Defense TOO Weak: 0.00%\n",
      "Total StyleCLIP Dataset Retention: 54.27%\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_defense_metrics(\"FGSM05\", fgsm05_attack_df, fgsm05_defend_df, fgsm05_detected_df)\n",
    "print_defense_metrics(\"FGSM10\", fgsm10_attack_df, fgsm10_defend_df, fgsm10_detected_df)\n",
    "print_defense_metrics(\"PGD1010\", pgd1010_attack_df, pgd1010_defend_df, pgd1010_detected_df)\n",
    "print_defense_metrics(\"PGD2010\", pgd2010_attack_df, pgd2010_defend_df, pgd2010_detected_df)\n",
    "print_defense_metrics(\"FGSM50\", fgsm50_attack_df, fgsm50_defend_df, fgsm50_detected_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
