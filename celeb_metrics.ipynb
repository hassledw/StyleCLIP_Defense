{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization and Metrics on the CelebA_HQ Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate Dataframes\n",
    "\n",
    "Each dataframe consists of 5 columns: `image`, `image_path`, `actual_label`, `predicted_label`, `confidence`\n",
    "* `image` is the image name, for example 10.jpg\n",
    "* `image_path` is the path to the image\n",
    "* `actual_label` is the actual label for that person (the original image)\n",
    "* `predicted_label` is the result of running the original, attacked, or defended image into the classifier. This is the label used to determine if the attack or defense was effective.\n",
    "* `confidence` is the confidence in which we think that the image belongs to that class. This is the max value of the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgsm05_attack_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/FGSM05.csv\")\n",
    "fgsm05_defend_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-FGSM05.csv\")\n",
    "fgsm05_detected_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-FGSM05-detected.csv\")\n",
    "\n",
    "fgsm10_attack_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/FGSM10.csv\")\n",
    "fgsm10_defend_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-FGSM10.csv\")\n",
    "fgsm10_detected_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-FGSM10-detected.csv\")\n",
    "\n",
    "fgsm50_attack_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/FGSM50.csv\")\n",
    "fgsm50_defend_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-FGSM50.csv\")\n",
    "fgsm50_detected_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-FGSM50-detected.csv\")\n",
    "\n",
    "pgd1010_attack_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/PGD1010.csv\")\n",
    "pgd1010_defend_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-PGD1010.csv\")\n",
    "pgd1010_detected_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-PGD1010-detected.csv\")\n",
    "\n",
    "pgd2010_attack_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/PGD2010.csv\")\n",
    "pgd2010_defend_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-PGD2010.csv\")\n",
    "pgd2010_detected_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-PGD2010-detected.csv\")\n",
    "\n",
    "pgd5050_attack_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/PGD5050.csv\")\n",
    "pgd5050_defend_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-PGD5050.csv\")\n",
    "pgd5050_detected_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-PGD5050-detected.csv\")\n",
    "\n",
    "jitter1010_attack_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/Jitter1010.csv\")\n",
    "jitter1010_defend_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-Jitter1010.csv\")\n",
    "jitter1010_detected_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-Jitter1010-detected.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defense Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_defense_metrics(attackname, attack_df, defense_df, detected_df):\n",
    "   total_images = len(attack_df)\n",
    "   \n",
    "   # Attacked image detection with StyleCLIP.\n",
    "   attack_detection = pd.merge(detected_df, defense_df, on=['image'], how='inner').drop(columns=['Unnamed: 0_x', 'image_path_x', 'Unnamed: 0_y', 'image_path_y',\n",
    "       'actual_label', 'predicted_label', 'confidence'])\n",
    "   \n",
    "   # Filter out detected images in defended_df dataset.\n",
    "   defense_df = pd.merge(detected_df, defense_df, on=['image'], how='outer')\n",
    "   defense_df = defense_df[defense_df.isna().any(axis=1)].drop(columns=['Unnamed: 0_x', 'image_path_x', 'Unnamed: 0_y'])\n",
    "   defense_df = defense_df.rename(columns={'image_path_y': 'image_path'})\n",
    "   # Successful attacks\n",
    "   attack_result = attack_df[attack_df[\"actual_label\"] != attack_df[\"predicted_label\"]]\n",
    "\n",
    "   # Successful defends inclusive of unsuccessful misclassfication in the attacks.\n",
    "   defense_result = defense_df[defense_df[\"actual_label\"] == defense_df[\"predicted_label\"]]\n",
    "\n",
    "   # Successful attacks and defenses joined. _x data is the attack, _y is the defense.\n",
    "   attack_defense_join = pd.merge(attack_result, defense_df, on=['image'], how='inner')\n",
    "\n",
    "   # Successfully reclassified images after successful attack.\n",
    "   attack_defense_success = attack_defense_join[attack_defense_join[\"actual_label_x\"] == attack_defense_join[\"predicted_label_y\"]]\n",
    "   \n",
    "   # Instances when the defense was too weak. When a successful attack happened, the defense generated an image with the same label as the attack.\n",
    "   attack_defense_weak = attack_defense_join[attack_defense_join[\"predicted_label_x\"] == attack_defense_join[\"predicted_label_y\"]]\n",
    "\n",
    "   attack_effectiveness = len(attack_result) / len(attack_df) * 100\n",
    "   styleclip_san_all = len(defense_result) / len(defense_df) * 100\n",
    "   styleclip_san_det_attack = len(attack_defense_success) / len(attack_defense_join) * 100\n",
    "   attack_detected_styleclip = len(attack_detection) / total_images * 100\n",
    "   styleclip_miss = 100 - (len(defense_result) / len(defense_df) * 100)\n",
    "   defense_too_weak = len(attack_defense_weak) / len(attack_defense_join) * 100\n",
    "   retention = (len(attack_detection) + len(defense_result)) / total_images * 100\n",
    "   styleclip_net_gain = retention - (100 - attack_effectiveness)\n",
    "\n",
    "   print(\"-\" * 50)\n",
    "   print(f\"RUNNING {attackname} DEFENSE METRICS...\\n\")\n",
    "   print(f\"Attack Effectiveness: {attack_effectiveness:.2f}%\\n\")\n",
    "\n",
    "   print(f\"StyleCLIP Sanitization Effectiveness (on ALL defendable images): {styleclip_san_all:.2f}%\")\n",
    "   print(f\"StyleCLIP Sanitization Effectiveness (on successful attacks): {styleclip_san_det_attack:.2f}%\")\n",
    "   print(f\"Attacked Image Detection w/ StyleCLIP: {attack_detected_styleclip:.2f}%\")\n",
    "   print(f\"Misclassification on StyleCLIP Images: {styleclip_miss:.2f}%\")\n",
    "   print(f\"Successful Attacks Where Defense TOO Weak: {defense_too_weak:.2f}%\")\n",
    "   print(f\"Total StyleCLIP Dataset Retention: {retention:.2f}%\")\n",
    "   print(f\"Net Gain Using StyleCLIP as a Defense: {styleclip_net_gain:.2f}%\")\n",
    "   print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "RUNNING FGSM05 DEFENSE METRICS...\n",
      "\n",
      "Attack Effectiveness: 27.14%\n",
      "\n",
      "StyleCLIP Sanitization Effectiveness (on ALL defendable images): 48.22%\n",
      "StyleCLIP Sanitization Effectiveness (on successful attacks): 9.62%\n",
      "Attacked Image Detection w/ StyleCLIP: 1.01%\n",
      "Misclassification on StyleCLIP Images: 51.78%\n",
      "Successful Attacks Where Defense TOO Weak: 46.15%\n",
      "Total StyleCLIP Dataset Retention: 48.74%\n",
      "Net Gain Using StyleCLIP as a Defense: -24.12%\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "RUNNING FGSM10 DEFENSE METRICS...\n",
      "\n",
      "Attack Effectiveness: 50.25%\n",
      "\n",
      "StyleCLIP Sanitization Effectiveness (on ALL defendable images): 27.55%\n",
      "StyleCLIP Sanitization Effectiveness (on successful attacks): 6.19%\n",
      "Attacked Image Detection w/ StyleCLIP: 1.51%\n",
      "Misclassification on StyleCLIP Images: 72.45%\n",
      "Successful Attacks Where Defense TOO Weak: 52.58%\n",
      "Total StyleCLIP Dataset Retention: 28.64%\n",
      "Net Gain Using StyleCLIP as a Defense: -21.11%\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "RUNNING FGSM50 DEFENSE METRICS...\n",
      "\n",
      "Attack Effectiveness: 96.48%\n",
      "\n",
      "StyleCLIP Sanitization Effectiveness (on ALL defendable images): 0.00%\n",
      "StyleCLIP Sanitization Effectiveness (on successful attacks): 0.00%\n",
      "Attacked Image Detection w/ StyleCLIP: 54.27%\n",
      "Misclassification on StyleCLIP Images: 100.00%\n",
      "Successful Attacks Where Defense TOO Weak: 0.00%\n",
      "Total StyleCLIP Dataset Retention: 54.27%\n",
      "Net Gain Using StyleCLIP as a Defense: 50.75%\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "RUNNING PGD1010 DEFENSE METRICS...\n",
      "\n",
      "Attack Effectiveness: 43.22%\n",
      "\n",
      "StyleCLIP Sanitization Effectiveness (on ALL defendable images): 34.69%\n",
      "StyleCLIP Sanitization Effectiveness (on successful attacks): 9.64%\n",
      "Attacked Image Detection w/ StyleCLIP: 1.51%\n",
      "Misclassification on StyleCLIP Images: 65.31%\n",
      "Successful Attacks Where Defense TOO Weak: 49.40%\n",
      "Total StyleCLIP Dataset Retention: 35.68%\n",
      "Net Gain Using StyleCLIP as a Defense: -21.11%\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "RUNNING PGD2010 DEFENSE METRICS...\n",
      "\n",
      "Attack Effectiveness: 75.38%\n",
      "\n",
      "StyleCLIP Sanitization Effectiveness (on ALL defendable images): 13.85%\n",
      "StyleCLIP Sanitization Effectiveness (on successful attacks): 4.79%\n",
      "Attacked Image Detection w/ StyleCLIP: 2.01%\n",
      "Misclassification on StyleCLIP Images: 86.15%\n",
      "Successful Attacks Where Defense TOO Weak: 40.41%\n",
      "Total StyleCLIP Dataset Retention: 15.58%\n",
      "Net Gain Using StyleCLIP as a Defense: -9.05%\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "RUNNING PGD5050 DEFENSE METRICS...\n",
      "\n",
      "Attack Effectiveness: 95.98%\n",
      "\n",
      "StyleCLIP Sanitization Effectiveness (on ALL defendable images): 0.00%\n",
      "StyleCLIP Sanitization Effectiveness (on successful attacks): 0.00%\n",
      "Attacked Image Detection w/ StyleCLIP: 70.35%\n",
      "Misclassification on StyleCLIP Images: 100.00%\n",
      "Successful Attacks Where Defense TOO Weak: 3.51%\n",
      "Total StyleCLIP Dataset Retention: 70.35%\n",
      "Net Gain Using StyleCLIP as a Defense: 66.33%\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "RUNNING Jitter1010 DEFENSE METRICS...\n",
      "\n",
      "Attack Effectiveness: 35.18%\n",
      "\n",
      "StyleCLIP Sanitization Effectiveness (on ALL defendable images): 42.13%\n",
      "StyleCLIP Sanitization Effectiveness (on successful attacks): 8.82%\n",
      "Attacked Image Detection w/ StyleCLIP: 1.01%\n",
      "Misclassification on StyleCLIP Images: 57.87%\n",
      "Successful Attacks Where Defense TOO Weak: 45.59%\n",
      "Total StyleCLIP Dataset Retention: 42.71%\n",
      "Net Gain Using StyleCLIP as a Defense: -22.11%\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_defense_metrics(\"FGSM05\", fgsm05_attack_df, fgsm05_defend_df, fgsm05_detected_df)\n",
    "print_defense_metrics(\"FGSM10\", fgsm10_attack_df, fgsm10_defend_df, fgsm10_detected_df)\n",
    "print_defense_metrics(\"FGSM50\", fgsm50_attack_df, fgsm50_defend_df, fgsm50_detected_df)\n",
    "\n",
    "print_defense_metrics(\"PGD1010\", pgd1010_attack_df, pgd1010_defend_df, pgd1010_detected_df)\n",
    "print_defense_metrics(\"PGD2010\", pgd2010_attack_df, pgd2010_defend_df, pgd2010_detected_df)\n",
    "print_defense_metrics(\"PGD5050\", pgd5050_attack_df, pgd5050_defend_df, pgd5050_detected_df)\n",
    "\n",
    "print_defense_metrics(\"Jitter1010\", jitter1010_attack_df, jitter1010_defend_df, jitter1010_detected_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
