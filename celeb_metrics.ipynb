{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization and Metrics on the CelebA_HQ Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate Dataframes\n",
    "\n",
    "Each dataframe consists of 5 columns: `image`, `image_path`, `actual_label`, `predicted_label`, `confidence`\n",
    "* `image` is the image name, for example 10.jpg\n",
    "* `image_path` is the path to the image\n",
    "* `actual_label` is the actual label for that person (the original image)\n",
    "* `predicted_label` is the result of running the original, attacked, or defended image into the classifier. This is the label used to determine if the attack or defense was effective.\n",
    "* `confidence` is the confidence in which we think that the image belongs to that class. This is the max value of the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgsm05_attack_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/FGSM05.csv\")\n",
    "fgsm05_defend_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-FGSM05.csv\")\n",
    "fgsm05_detected_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-FGSM05-detected.csv\")\n",
    "\n",
    "fgsm10_attack_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/FGSM10.csv\")\n",
    "fgsm10_defend_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-FGSM10.csv\")\n",
    "fgsm10_detected_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-FGSM10-detected.csv\")\n",
    "\n",
    "fgsm50_attack_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/FGSM50.csv\")\n",
    "fgsm50_defend_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-FGSM50.csv\")\n",
    "fgsm50_detected_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-FGSM50-detected.csv\")\n",
    "\n",
    "pgd1010_attack_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/PGD1010.csv\")\n",
    "pgd1010_defend_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-PGD1010.csv\")\n",
    "pgd1010_detected_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-PGD1010-detected.csv\")\n",
    "\n",
    "pgd2010_attack_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/PGD2010.csv\")\n",
    "pgd2010_defend_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-PGD2010.csv\")\n",
    "pgd2010_detected_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-PGD2010-detected.csv\")\n",
    "\n",
    "pgd5050_attack_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/PGD5050.csv\")\n",
    "pgd5050_defend_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-PGD5050.csv\")\n",
    "pgd5050_detected_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-PGD5050-detected.csv\")\n",
    "\n",
    "jitter1010_attack_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/Jitter1010.csv\")\n",
    "jitter1010_defend_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-Jitter1010.csv\")\n",
    "jitter1010_detected_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-Jitter1010-detected.csv\")\n",
    "\n",
    "test_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/test.csv\")\n",
    "test_defended_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-test.csv\")\n",
    "test_detected_df = pd.read_csv(\"/home/grads/hassledw/StyleCLIP_Defense/CelebA_HQ-Labeled/StyleCLIP-test-detected.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defense Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_test_metrics(test_df, test_defended_df, test_detected_df):\n",
    "   test_defended_df[\"predicted_label_test\"] = test_df[\"predicted_label\"]\n",
    "   \n",
    "   face_detection = pd.merge(test_detected_df, test_defended_df, on=['image'], how='inner').drop(columns=['Unnamed: 0_x', 'image_path_x', 'Unnamed: 0_y', 'image_path_y',\n",
    "   'actual_label', 'predicted_label', 'confidence'])\n",
    "\n",
    "   defense_df = pd.merge(test_detected_df, test_defended_df, on=['image'], how='outer')\n",
    "   defense_df = defense_df[defense_df.isna().any(axis=1)].drop(columns=['Unnamed: 0_x', 'image_path_x', 'Unnamed: 0_y'])\n",
    "   defense_df = defense_df.rename(columns={'image_path_y': 'image_path'})\n",
    "\n",
    "   accuracy_orig_df = test_df[test_df[\"actual_label\"] == test_df[\"predicted_label\"]]   \n",
    "   accuracy_df = defense_df[defense_df[\"predicted_label_test\"] == defense_df[\"predicted_label\"]]\n",
    "   \n",
    "   total_images = len(test_df)\n",
    "   total_defendable_images = len(defense_df)\n",
    "\n",
    "   accuracy = len(accuracy_df) / total_defendable_images * 100\n",
    "   accuracy_orig = len(accuracy_orig_df) / total_images * 100\n",
    "   # accuracy on classifier\n",
    "   print(f\"Accuracy of classifier on original test set {accuracy_orig:.2f}%\")\n",
    "   # A = C\n",
    "   print(f\"Accuracy on StyleCLIP-test {accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "   # img 19 = acutal_label: 5\n",
    "   # classifier(img 19) = predicted_label : 20\n",
    "\n",
    "   # classifier(styleClIP(img 19)) = predicted_label : 20\n",
    "   # compare(styleCLIP(img 19), predicted_label)\n",
    "   # - does 20 = 20? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of classifier on original test set 81.41%\n",
      "Accuracy on StyleCLIP-test 68.53%\n"
     ]
    }
   ],
   "source": [
    "print_test_metrics(test_df, test_defended_df, test_detected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defense Metrics w/o classifier errors on original test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_defense_metrics(attackname, attack_df, defense_df, detected_df, test_df=test_df, use_predicted=True):\n",
    "   total_images = len(attack_df)\n",
    "\n",
    "   # Add predicted_labels of the test set to adjust for innate classifier errors since classifier is only 80% accurate.\n",
    "   attack_df[\"predicted_label_test\"] = test_df[\"predicted_label\"]\n",
    "   defense_df[\"predicted_label_test\"] = test_df[\"predicted_label\"]\n",
    "   \n",
    "   label_comp = \"actual_label\"\n",
    "   if use_predicted:\n",
    "      label_comp = \"predicted_label_test\"\n",
    "   \n",
    "   # Attacked image detection with StyleCLIP.\n",
    "   face_detection = pd.merge(detected_df, defense_df, on=['image'], how='inner').drop(columns=['Unnamed: 0_x', 'image_path_x', 'Unnamed: 0_y', 'image_path_y',\n",
    "       'actual_label', 'predicted_label', 'confidence'])\n",
    "   \n",
    "   # Filter out face undetected images in defended_df dataset.\n",
    "   defense_df = pd.merge(detected_df, defense_df, on=['image'], how='outer')\n",
    "   defense_df = defense_df[defense_df.isna().any(axis=1)].drop(columns=['Unnamed: 0_x', 'image_path_x', 'Unnamed: 0_y'])\n",
    "   defense_df = defense_df.rename(columns={'image_path_y': 'image_path'})\n",
    "\n",
    "   # Successful attacks\n",
    "   attack_result = attack_df[attack_df[label_comp] != attack_df[\"predicted_label\"]]\n",
    "\n",
    "   # Successful defends inclusive of unsuccessful misclassfication in the attacks.\n",
    "   defense_result = defense_df[defense_df[label_comp] == defense_df[\"predicted_label\"]]\n",
    "\n",
    "   # Successful attacks and defenses joined. _x data is the attack, _y is the defense.\n",
    "   attack_defense_join = pd.merge(attack_result, defense_df, on=['image'], how='inner')\n",
    "\n",
    "   # Successfully reclassified images after successful attack.\n",
    "   attack_defense_success = attack_defense_join[attack_defense_join[f\"{label_comp}_x\"] == attack_defense_join[\"predicted_label_y\"]]\n",
    "   \n",
    "   # Instances when the defense was too weak. When a successful attack happened, the defense generated an image with the same label as the attack.\n",
    "   attack_defense_weak = attack_defense_join[attack_defense_join[\"predicted_label_x\"] == attack_defense_join[\"predicted_label_y\"]]\n",
    "\n",
    "   # StyleCLIP noticeable classification change. If an attacked image is classified as something else by StyleCLIP, this shows the image is attacked.\n",
    "   class_change = pd.merge(attack_df, defense_df, on=['image'], how='inner').drop(columns=['Unnamed: 0', 'image_path_x', 'actual_label_x',\n",
    "        'confidence_x', 'image_path_y', 'actual_label_y', 'confidence_y'])\n",
    "   class_change = class_change[class_change['predicted_label_x'] != class_change['predicted_label_y']]\n",
    "   \n",
    "   attack_effectiveness = len(attack_result) / len(attack_df) * 100\n",
    "   styleclip_san_all = len(defense_result) / len(defense_df) * 100\n",
    "   styleclip_san_det_attack = len(attack_defense_success) / len(attack_defense_join) * 100\n",
    "\n",
    "   attack_face_detected_styleclip = len(face_detection) / total_images * 100\n",
    "   attack_class_change_styleclip = len(class_change) /total_images * 100\n",
    "   attack_detected_styleclip = (len(face_detection) + len(class_change)) / total_images * 100\n",
    "\n",
    "   styleclip_miss = 100 - (len(defense_result) / len(defense_df) * 100)\n",
    "   defense_too_weak = len(attack_defense_weak) / len(attack_defense_join) * 100\n",
    "   retention = (attack_detected_styleclip + len(defense_result)) / total_images * 100\n",
    "   styleclip_net_gain = retention - (100 - attack_effectiveness)\n",
    "\n",
    "   print(\"-\" * 50)\n",
    "   print(f\"RUNNING {attackname} DEFENSE METRICS...\\n\")\n",
    "   print(f\"Attack Effectiveness: {attack_effectiveness:.2f}%\\n\")\n",
    "\n",
    "   # A = C\n",
    "   print(f\"StyleCLIP Sanitization Effectiveness (on ALL defendable images): {styleclip_san_all:.2f}%\")\n",
    "   # A != B and A = C\n",
    "   print(f\"StyleCLIP Sanitization Effectiveness (on successful attacks): {styleclip_san_det_attack:.2f}%\")\n",
    "\n",
    "   # Face detect, shape error thrown, stored in detected_df.\n",
    "   # original classification: A = C(A)\n",
    "   # classification change: B != C\n",
    "   print()\n",
    "   print(f\"Attacked Image Detection w/ StyleCLIP: {attack_detected_styleclip:.2f}%\")\n",
    "   print(f\"\\t Couldn't Detect a Face: {attack_face_detected_styleclip:.2f}%\")\n",
    "   print(f\"\\t Classification Change: {attack_class_change_styleclip:.2f}%\")\n",
    "   print()\n",
    "\n",
    "   # A != C\n",
    "   print(f\"Misclassification on StyleCLIP Images: {styleclip_miss:.2f}%\")\n",
    "   # A != B and B = C\n",
    "   print(f\"Successful Attacks Where Sanitization TOO Weak: {defense_too_weak:.2f}%\")\n",
    "\n",
    "   # print(f\"Total StyleCLIP Dataset Retention: {retention:.2f}%\")\n",
    "   \n",
    "   # (sum(defended) / total_images) - (100 - attack_effectiveness)\n",
    "   print(f\"Net Gain Using StyleCLIP as a Defense: {styleclip_net_gain:.2f}%\")\n",
    "   print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "RUNNING FGSM05 DEFENSE METRICS...\n",
      "\n",
      "Attack Effectiveness: 19.10%\n",
      "\n",
      "StyleCLIP Sanitization Effectiveness (on ALL defendable images): 51.78%\n",
      "StyleCLIP Sanitization Effectiveness (on successful attacks): 10.53%\n",
      "\n",
      "Attacked Image Detection w/ StyleCLIP: 42.71%\n",
      "\t Couldn't Detect a Face: 1.01%\n",
      "\t Classification Change: 41.71%\n",
      "\n",
      "Misclassification on StyleCLIP Images: 48.22%\n",
      "Successful Attacks Where Sanitization TOO Weak: 42.11%\n",
      "Net Gain Using StyleCLIP as a Defense: -8.18%\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "RUNNING FGSM10 DEFENSE METRICS...\n",
      "\n",
      "Attack Effectiveness: 46.73%\n",
      "\n",
      "StyleCLIP Sanitization Effectiveness (on ALL defendable images): 30.61%\n",
      "StyleCLIP Sanitization Effectiveness (on successful attacks): 7.69%\n",
      "\n",
      "Attacked Image Detection w/ StyleCLIP: 50.25%\n",
      "\t Couldn't Detect a Face: 1.51%\n",
      "\t Classification Change: 48.74%\n",
      "\n",
      "Misclassification on StyleCLIP Images: 69.39%\n",
      "Successful Attacks Where Sanitization TOO Weak: 50.55%\n",
      "Net Gain Using StyleCLIP as a Defense: 2.14%\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "RUNNING FGSM50 DEFENSE METRICS...\n",
      "\n",
      "Attack Effectiveness: 95.48%\n",
      "\n",
      "StyleCLIP Sanitization Effectiveness (on ALL defendable images): 0.00%\n",
      "StyleCLIP Sanitization Effectiveness (on successful attacks): 0.00%\n",
      "\n",
      "Attacked Image Detection w/ StyleCLIP: 100.00%\n",
      "\t Couldn't Detect a Face: 54.27%\n",
      "\t Classification Change: 45.73%\n",
      "\n",
      "Misclassification on StyleCLIP Images: 100.00%\n",
      "Successful Attacks Where Sanitization TOO Weak: 0.00%\n",
      "Net Gain Using StyleCLIP as a Defense: 45.73%\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "RUNNING PGD1010 DEFENSE METRICS...\n",
      "\n",
      "Attack Effectiveness: 37.19%\n",
      "\n",
      "StyleCLIP Sanitization Effectiveness (on ALL defendable images): 38.27%\n",
      "StyleCLIP Sanitization Effectiveness (on successful attacks): 12.50%\n",
      "\n",
      "Attacked Image Detection w/ StyleCLIP: 49.25%\n",
      "\t Couldn't Detect a Face: 1.51%\n",
      "\t Classification Change: 47.74%\n",
      "\n",
      "Misclassification on StyleCLIP Images: 61.73%\n",
      "Successful Attacks Where Sanitization TOO Weak: 48.61%\n",
      "Net Gain Using StyleCLIP as a Defense: -0.38%\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "RUNNING PGD2010 DEFENSE METRICS...\n",
      "\n",
      "Attack Effectiveness: 70.85%\n",
      "\n",
      "StyleCLIP Sanitization Effectiveness (on ALL defendable images): 17.44%\n",
      "StyleCLIP Sanitization Effectiveness (on successful attacks): 6.47%\n",
      "\n",
      "Attacked Image Detection w/ StyleCLIP: 60.30%\n",
      "\t Couldn't Detect a Face: 2.01%\n",
      "\t Classification Change: 58.29%\n",
      "\n",
      "Misclassification on StyleCLIP Images: 82.56%\n",
      "Successful Attacks Where Sanitization TOO Weak: 38.85%\n",
      "Net Gain Using StyleCLIP as a Defense: 18.24%\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "RUNNING PGD5050 DEFENSE METRICS...\n",
      "\n",
      "Attack Effectiveness: 94.97%\n",
      "\n",
      "StyleCLIP Sanitization Effectiveness (on ALL defendable images): 0.00%\n",
      "StyleCLIP Sanitization Effectiveness (on successful attacks): 0.00%\n",
      "\n",
      "Attacked Image Detection w/ StyleCLIP: 98.99%\n",
      "\t Couldn't Detect a Face: 70.35%\n",
      "\t Classification Change: 28.64%\n",
      "\n",
      "Misclassification on StyleCLIP Images: 100.00%\n",
      "Successful Attacks Where Sanitization TOO Weak: 3.64%\n",
      "Net Gain Using StyleCLIP as a Defense: 44.72%\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "RUNNING Jitter1010 DEFENSE METRICS...\n",
      "\n",
      "Attack Effectiveness: 27.14%\n",
      "\n",
      "StyleCLIP Sanitization Effectiveness (on ALL defendable images): 46.19%\n",
      "StyleCLIP Sanitization Effectiveness (on successful attacks): 9.43%\n",
      "\n",
      "Attacked Image Detection w/ StyleCLIP: 45.73%\n",
      "\t Couldn't Detect a Face: 1.01%\n",
      "\t Classification Change: 44.72%\n",
      "\n",
      "Misclassification on StyleCLIP Images: 53.81%\n",
      "Successful Attacks Where Sanitization TOO Weak: 41.51%\n",
      "Net Gain Using StyleCLIP as a Defense: -4.16%\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_defense_metrics(\"FGSM05\", fgsm05_attack_df, fgsm05_defend_df, fgsm05_detected_df)\n",
    "print_defense_metrics(\"FGSM10\", fgsm10_attack_df, fgsm10_defend_df, fgsm10_detected_df)\n",
    "print_defense_metrics(\"FGSM50\", fgsm50_attack_df, fgsm50_defend_df, fgsm50_detected_df)\n",
    "\n",
    "print_defense_metrics(\"PGD1010\", pgd1010_attack_df, pgd1010_defend_df, pgd1010_detected_df)\n",
    "print_defense_metrics(\"PGD2010\", pgd2010_attack_df, pgd2010_defend_df, pgd2010_detected_df)\n",
    "print_defense_metrics(\"PGD5050\", pgd5050_attack_df, pgd5050_defend_df, pgd5050_detected_df)\n",
    "\n",
    "print_defense_metrics(\"Jitter1010\", jitter1010_attack_df, jitter1010_defend_df, jitter1010_detected_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
